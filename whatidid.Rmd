---
title: "Philadelphia"
author: "Zhuyi Huang"
output: pdf_document
---
```{r library, message=FALSE, warning=FALSE}
library(tidyverse)
library(tsbox) # transform data into time series
library(xts)
library(COVID19) # to get data about covid 19
library(forecast) #arima model
library(vars) #VAR and Causality
```

# Philadelphia
```{r}
phil <- read.csv('/Users/chelseahuang/Desktop/data/phil_crime/phil_crime.csv')
```

```{r}
# add YEAR, MONTH
phil <- phil %>%
  mutate(y_month = substr(dispatch_date_time, start = 1, stop = 7)) %>%
  mutate(YEAR = substr(dispatch_date_time, start = 1, stop = 4)) %>%
  mutate(MONTH = substr(dispatch_date_time, start = 6, stop = 7))
```


```{r}
# summary of all crime
phil_summary <- phil %>%
  group_by(text_general_code) %>%
  summarise(number_of_crime = n()) %>%
  arrange(desc(number_of_crime))

# bar chart of 7 most frequent crime over the years
phil %>%
  filter(text_general_code %in% head(phil_summary$text_general_code, 7)) %>%
  ggplot(aes(x=text_general_code, fill=text_general_code)) +
  geom_bar(width = 0.5) +
  coord_flip() +
  theme_classic() +
  labs(y='Number of Crimes',x='Offense Description')

# top 7 crime over time
# monthly
# exclude May due to incomplete info
phil %>%
  dplyr::select(y_month, text_general_code) %>%
  filter(text_general_code %in% phil_summary$text_general_code[1:7], y_month != "2020-05") %>% 
  count(y_month, text_general_code) %>%
  ggplot(aes(y_month, n, group = text_general_code, fill = text_general_code)) +
  geom_area(size = 0.01, alpha = 0.5) + 
  scale_fill_brewer(palette = "Set1", breaks = rev(levels(phil_summary$text_general_code[1:7]))) +
  ggtitle("Frequency of top 7 crime in phil since 2015")

# top 7 crime in 2020
# area chart
phil %>%
  dplyr::select(dispatch_date, text_general_code, YEAR) %>%
  filter(text_general_code %in% phil_summary$text_general_code[1:7], YEAR == 2020) %>% 
  count(dispatch_date, text_general_code) %>%
  ggplot(aes(dispatch_date, n, group = text_general_code, fill = text_general_code)) +
  geom_area(size = 0.01, alpha = 0.4) +
  scale_fill_brewer(palette = "Set1", breaks = rev(levels(phil_summary$text_general_code[1:7]))) +
  ggtitle("Frequency of top 7 crime in phil in 2020")

# line
# per day
phil %>%
  dplyr::select(dispatch_date, text_general_code, YEAR) %>%
  filter(text_general_code %in% phil_summary$text_general_code[1:7], YEAR == 2020) %>% 
  count(dispatch_date, text_general_code) %>%
  ggplot(aes(dispatch_date, n, group = text_general_code, color = text_general_code)) +
  geom_line() +
  scale_color_brewer(palette = "Set1") +
  ggtitle("Frequency of top 7 crime in phil in 2020")


# per month
# exclude May
phil %>%
  dplyr::select(MONTH, text_general_code, YEAR) %>%
  filter(text_general_code %in% phil_summary$text_general_code[1:7], YEAR == 2020, MONTH != 05) %>% 
  count(MONTH, text_general_code) %>%
  ggplot(aes(MONTH, n, group = text_general_code, color = text_general_code)) +
  geom_line() +
  scale_color_brewer(palette = "Set1") +
  ggtitle("Monthly frequency of top 7 crime in phil in 2020")

# This dataset doesn't have 'day of the week' yet.
```

## VAR 
### Step1 : Extract cases
```{r extract cases}
# extract top 5 crime
top5crime <- phil %>%
  filter(text_general_code %in% head(phil_summary$text_general_code, 5)) %>%
  group_by(dispatch_date, text_general_code) %>%
  tally() %>%
  spread(text_general_code, n)

# rename columns
colnames(top5crime) <- c('time',
                         "offense",
                         "assault",
                         "vehicle",
                         "thefts",
                         "vandalism")

# create time series
top5crime_xts <- ts_xts(top5crime[,1:2])

for (i in (3:ncol(top5crime))){
  temp_xts <- ts_xts(top5crime[, c(1,i)])
  top5crime_xts <- merge(top5crime_xts, temp_xts)
}

# extract difference, change per day
top5crime_diff <- na.omit(diff(top5crime_xts))
```
### Step 2: Construct combined time series
## COVID 19 RELATED
```{r covid 19 extract Pennsylvania data}
# extract Pennsylvania data from US data. 
covid19_PA <- covid19("USA", level = 2) %>%
  filter(administrative_area_level_2 == "Pennsylvania") %>%
  # filter out days when confirmed is zero
  filter(confirmed > 0)

# calculate the difference per day
covid19_PA_diff <- data.frame(diff(covid19_PA$confirmed))
colnames(covid19_PA_diff)[1] = "confirmed"
covid19_PA_diff$date = covid19_PA$date[2:length(covid19_PA$date)]

head(covid19_PA)
```

```{r covid 19 related exploration}
# extract for tranforming into time series data
ts_PA <- covid19_PA %>% 
  dplyr::select(date, confirmed) %>%
  ts_xts()

# plot time series of PA infection
ts_plot(ts_PA)
# conduct ADF Test
adf.test(as.ts(ts_PA))
# not stationary!

# try first log difference
ts_diff_PA <- diff(ts_PA)
ts_plot(ts_diff_PA)
# still clearly not stationary
# need de-trend

# de-trend 
# GAMM model from STA303 A3

# time as integer
covid19_PA_diff$timeInt = as.numeric(covid19_PA_diff$date)
# make a copy to avoid perfect collinearity
covid19_PA_diff$timeIid = covid19_PA_diff$timeInt

# make a copy to avoid perfect collinearity
covid19_PA_diff$timeIid = covid19_PA_diff$timeInt

# GAMM model
# 50 too overfit. 15 looks decent
gamPA <- gamm4::gamm4(confirmed ~  s(timeInt, k=50), random = ~(1|timeIid), 
	data=covid19_PA_diff, family=poisson(link='log'))

lme4::VarCorr(gamPA$mer)
# looks like random intercept is making little difference.
# choose to not have random effect to preserve it for time series analysis

# plot fitted value
toPredict = data.frame(time = seq(covid19_PA_diff$date[1], 
                                          covid19_PA_diff$date[length(covid19_PA_diff$date)],
                                  by = '1 day'))
toPredict$timeInt = as.numeric(toPredict$time)

# obtain forecast
forecast <- data.frame(exp(do.call(cbind, mgcv::predict.gam(gamPA$gam, toPredict, se.fit=TRUE))))

# access residuals
PA_res <- data.frame(covid19_PA_diff$confirmed - forecast$fit)

# transform into time series
PA_res$time = covid19_PA_diff$date
colnames(PA_res)[1] = "residuals"

col_order <- c("time", "residuals")
PA_res <- PA_res[, col_order]

PA_res_ts <- ts_xts(PA_res)

plot.xts(PA_res_ts)
# adf test
adf.test(as.ts(PA_res_ts))
# Stationary process
```

```{r top 5 crime VAR}
# specify common time range
# start from when covid was a thing
# end with crime since it is manually updated
common_time <- seq.Date(start(PA_res_ts), end(top5crime_diff), by = "day")

# combine time series of crime and covid
combined_diff <- merge(top5crime_diff[paste(common_time[1],
                                            common_time[length(common_time)],
                                            sep = "/")],
                       PA_res_ts[paste(common_time[1],
                                            common_time[length(common_time)],
                                            sep = "/")])

```

### Step 3: Plot each crime with covid
```{r plot together}
for (i in 1:(ncol(combined_diff) - 1)){
  plotrix::twoord.plot(common_time,
                       combined_diff[,i],
                       common_time,
                       combined_diff$residuals,
                       type = c("l","l"),
                       xaxt = "n",
                       rylab = "number of daily fluctuation of covid 19 cases",
                       ylab = paste("daily change in", colnames(combined_diff)[i]))
                       
}
```
### Step 5: Construct VAR model
```{r construct var}
optimal_offense <- VARselect(na.omit(combined_diff)[,c(1,6)], type = 'none', lag.max = 10)
optimal_assault <- VARselect(na.omit(combined_diff)[,c(2,6)], type = 'none', lag.max = 10)
optimal_vehicle <- VARselect(na.omit(combined_diff)[,c(3,6)], type = 'none', lag.max = 10)
optimal_thefts <- VARselect(na.omit(combined_diff)[,c(4,6)], type = 'none', lag.max = 10)
optimal_vandalism <- VARselect(na.omit(combined_diff)[,c(5,6)], type = 'none', lag.max = 10)

VAR_offense <- VAR(y=as.ts(na.omit(combined_diff)[,c(1,6)]), p=optimal_offense$selection[1])
VAR_assault <- VAR(y=as.ts(na.omit(combined_diff)[,c(2,6)]), p=optimal_assault$selection[1])
VAR_vehicle <- VAR(y=as.ts(na.omit(combined_diff)[,c(3,6)]), p=optimal_vehicle$selection[1])
VAR_thefts <- VAR(y=as.ts(na.omit(combined_diff)[,c(4,6)]), p=optimal_thefts$selection[1])
VAR_vandalism <- VAR(y=as.ts(na.omit(combined_diff)[,c(5,6)]),
                     p=optimal_vandalism$selection[1])
```

### Step 6: Granger Causality test

```{r granger offense}
# offense
causality(VAR_offense, cause = colnames(combined_diff)[1])
causality(VAR_offense, cause = "residuals")
```
instantaneous causality between: offense and residuals (p-value = 0.04408)

```{r granger assault}
causality(VAR_assault, cause = colnames(combined_diff)[2])
causality(VAR_assault, cause = "residuals")

# diagnostic testing
serial.test(VAR_assault) # no autocorrelation in errors
arch.test(VAR_assault)
normality.test(VAR_assault) # not normal
```
```{r granger vehicle}
causality(VAR_vehicle, cause = colnames(combined_diff)[3])
causality(VAR_vehicle, cause = "residuals")

# diagnostic testing
serial.test(VAR_vehicle) # autocorrelation in errors
arch.test(VAR_vehicle)
normality.test(VAR_vehicle)
```
COVID-19 may help to predict thefts from vehicle.(p-value = 0.0625)
Instantaneous causality appears when we include the current information of variables (p-value = 0.02328)

```{r granger thefts}
causality(VAR_thefts, cause = colnames(combined_diff)[4])
causality(VAR_thefts, cause = "residuals")
```

```{r granger vandalism}
causality(VAR_vandalism, cause = colnames(combined_diff)[5])
causality(VAR_vandalism, cause = "residuals")
```
instantaneous causality between: vandalism and residuals (p-value = 0.03222)


### Step 7: Impulse Response Function

Only thefts from vehicle may have granger causality.

```{r irf}
par(mfrow = c(1,2))
# vehicle theft
irf_vehicle_1 <- irf(VAR_vehicle, 
                         impulse = "vehicle", 
                         response = "residuals", 
                         n.ahead = 24)
irf_vehicle_2 <- irf(VAR_vehicle, 
                         impulse = "residuals", 
                         response = "vehicle", 
                         n.ahead = 24)
plot(irf_vehicle_1)
plot(irf_vehicle_2)

```

### Step 8: Forecast
```{r var forecast}
forecast(VAR_vehicle) %>%
  autoplot()

accu_compare <- data.frame(rbind(accuracy(VAR_vehicle$varresult[[1]]),
                                 accuracy(VAR_thefts$varresult[[1]]),
                                 accuracy(VAR_vandalism$varresult[[1]])))
rownames(accu_compare) <- c('vehicle', 'thefts', 'vandalism')
kableExtra::kable(accu_compare, format = 'markdown')
```

### CONCLUSION (Last update: MAY 30) 
Slightly non-significant contribution on prediction:
  thefts from vehicle (p-value=0.0625)

Significant simultaneous movement:
 	 Offense (p-value = 0.04408), thefts from vehicle (p-value = 0.02328), 
vandalism (p-value = 0.03222)

  
==================================

# Boston Crime
## Load crime data

```{r crime data info}
length(unique(boston_summary$OFFENSE_DESCRIPTION))
# number of crime categories: 283
length(unique(boston$DISTRICT))
# number of district: 14

boston %>%
  ggplot(aes(x=DAY_OF_WEEK,fill=DAY_OF_WEEK)) +
  geom_bar(width=0.5) + 
  theme_classic() +
  labs(y='Number of Crimes',x='Day of Week')

```

```{r}
# bar chart of 5 most frequent crime over the years
boston %>%
  filter(OFFENSE_DESCRIPTION %in% head(boston_summary$OFFENSE_DESCRIPTION, 5)) %>%
  ggplot(aes(x=OFFENSE_DESCRIPTION,fill=OFFENSE_DESCRIPTION)) +
  geom_bar(width=0.5) +
  coord_flip() +
  theme_classic() +
  labs(y='Number of Crimes',x='Offense Description')
```

### Univariate Model
#### Simple AR model
```{r univariate model}
# white noise test
for (i in 1:2) {
  print(Box.test(as.ts(dispute_diff_xts), type='Ljung-Box',lag=6*i))
}

# plot ACF and PACF
acf(as.ts(dispute_diff_xts))
pacf(as.ts(dispute_diff_xts))

# k = 3 for PACF
dispute_ar <- arima(as.ts(dispute_diff_xts), order = c(3,0,0))
dispute_ar

dispute_ar <- arima(as.ts(dispute_diff_xts), order = c(3,0,1)) #ARMA(3,1) is better than above
dispute_ar

# p = 10 for comparison against VAR
dispute_ar_10 <- arima(as.ts(dispute_diff_xts), order = c(10,0,0))
dispute_ar_10

# auto arima model
dispute_ar_auto <- forecast::auto.arima(as.ts(dispute_diff_xts)) 
dispute_ar_auto

# since auto-arima says fit an ARIMA(0,0,1) model
dispute_diff_xts.fit <- arima(as.ts(dispute_diff_xts), order=c(0,0,1))
dispute_diff_xts.fit
for (i in 1:2) {
  print(Box.test(dispute_diff_xts.fit$residual, type='Ljung-Box',lag=6*i))
}

residual <- dispute_diff_xts.fit$resid
par(mfrow=c(1,2))
acf(residual)
pacf(residual)
Box.test(residual, type='Ljung-Box') #p-value=0.44, fit was good

par(mfrow=c(1,2))
acf(residual^2)
pacf(residual^2)
Box.test(residual^2, type='Ljung-Box') 
# the squares of residuals are a sequence of white noise; 
# the residuals are homoscedastic; No ARCH effect
# The Ljung-box test confirms that the residuals do not have heteroscedasticity because the p-values are very large.
```
```{r ARCH test, eval=F}
library(aTSA)
aTSA::arch.test(dispute_diff_xts.fit)
# The plots of residuals, squared residuals, p.values of PQ and LM tests 
# Although, the p-values of Portmanteau test are pretty high
# Lagrange multiplier test coefficients are significant, at least until lag 8

library(FinTS)
dispute_archTest <- ArchTest(residual,lags=12)
dispute_archTest

#we reject the null hypothesis and conclude the presence of ARCH(12) effects.
```

```{r bayesGARCH, eval=F}
library(bayesGARCH)
MCMC <- bayesGARCH(as.numeric(as.ts((dispute_diff_xts))), control = list(n.chain=2, l.chain=100))
plot(MCMC)
```

```{r EXP SMOOTHING}
#simple exponential smoothing for default ts assuming no trend
dispute_forecasts <- HoltWinters(as.ts(dispute_diff_xts), beta=F, gamma=F,l.start = -10)
dispute_forecasts
plot(dispute_forecasts)
dispute_forecasts$SSE
# sum of squared error = 3657.46

dispute_fitets <- ets(as.ts(dispute_diff_xts))
checkresiduals(dispute_fitets)
summary(dispute_fitets)
autoplot(forecast.ets(dispute_fitets))
# "ANN" is simple exponential smoothing with additive errors

library(forecast)
# predicting for next 14 days
dispute_forecasts2 <- forecast:::forecast.HoltWinters(dispute_forecasts, h=14)
#dispute_forecasts2
forecast:::plot.forecast(dispute_forecasts2)
dispute_forecasts2$residuals # 1 NA exist
b <- dispute_forecasts2$residuals[-which(is.na(dispute_forecasts2$residuals))]
acf(b,plot=T)
Box.test(dispute_forecasts2$residual, type="Ljung-Box") # not indep

plot(forecast(dispute_ar_auto, bootstrap = T))

forecast::checkresiduals(dispute_ar_auto, test = F)
forecast::checkresiduals(dispute_ar_auto, plot = F)
```

```{r}
plotForecastErrors <- function(forecasterrors)
  {
     # make a histogram of the forecast errors:
     mybinsize <- IQR(forecasterrors, na.rm = T)/4
     mysd   <- sd(forecasterrors, na.rm = T)
     mymin  <- min(forecasterrors, na.rm = T) - mysd*5
     mymax  <- max(forecasterrors, na.rm = T) + mysd*3
     # generate normally distributed data with mean 0 and standard deviation mysd
     mynorm <- rnorm(10000, mean=0, sd=mysd)
     mymin2 <- min(mynorm)
     mymax2 <- max(mynorm)
     if (mymin2 < mymin) { mymin <- mymin2 }
     if (mymax2 > mymax) { mymax <- mymax2 }
     # make a red histogram of the forecast errors, with the normally distributed data overlaid:
     mybins <- seq(mymin, mymax, mybinsize)
     hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
     # freq=FALSE ensures the area under the histogram = 1
     # generate normally distributed data with mean 0 and standard deviation mysd
     myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
     # plot the normal curve as a blue line on top of the histogram of forecast errors:
     points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}
#The plot shows that the distribution of forecast errors is roughly centred on zero, and is more or less normally distributed.
#The Ljung-Box test showed that there is little evidence of non-zero autocorrelations in the in-sample forecast errors, and the distribution of forecast errors seems to be normally distributed with mean zero. 
# This suggests that the simple exponential smoothing method provides an adequate predictive model for water usage. 
# Furthermore, the assumptions that the 80% and 95% predictions intervals were based upon
#(that there are no autocorrelations in the forecast errors, and the forecast errors are normally distributed with mean zero and constant variance) are probably valid.

plotForecastErrors(dispute_forecasts2$residuals)
```

```{r CROSS VALIDATION, eval=F}
#---------Cross validation---------------
#auto.arima forecast function for time series cross validation
autoplot(forecast(dispute_ar_auto, bootstrap = T))

library(zoo)
z <- read.zoo(dispute[])
training <- window(z, start='2017-12-01', end='2020-05-17')
testset <- window(z, start='2015-06-15', end='2017-11-29')

training <- window(as.ts(dispute_diff_xts), end=39)
testset <- window(as.ts(dispute_diff_xts), start=40)

fc <- function(y, h, xreg)
{
  X <- window(xreg, start = 1, end = length(y))
  #X <- xreg[1:length(y),]
  if(NROW(xreg) < length(y) + h)
    stop("Not enough xreg data for forecasting")
  #newX <- window(xreg, start = length(y)+(1:h))
  #newX <- xreg[length(y)+(1:h),]
  fit <- auto.arima(y, xreg=X)
  forecast(fit, xreg=newX)
}

e <- tsCV(y=training, fc, xreg=testset)
# error: results in NAs 

autoplot(e)
sqrt(mean(e^2,na.rm = T))
```


```{r}
var_dispute.stabil <- vars::stability(x=VAR_dispute, type ="OLS-CUSUM", h = 0.15, dynamic = FALSE, rescale = TRUE)

plot(var_dispute.stabil)
# VAR is stable
```

```{r VAR}
# select the optimal lag number within 10
VARselect(na.omit(combined_diff), lag.max = 10, type = 'const')
# LET'S USE 8

# VAR model estimation
VAR_dispute <- vars::VAR(y=na.omit(combined_diff), p = 8)
```

```{r FORECASTS FROM MODELS SUMMARY}
dispute_struct <- StructTS(as.ts(dispute_diff_xts))
dispute_struct_forec <- forecast(dispute_struct, h=14)

# all forcast for 14 days
par(mfrow=c(6,1))
par(mar=c(1,1,1,1))
plot(forecast(dispute_ar_auto,14, bootstrap = T))
plot(forecast(dispute_diff_xts.fit))
plot(forecast(dispute_fitets))
forecast:::plot.forecast(dispute_forecasts2)
plot(dispute_annm)
plot(dispute_struct_forec)
```

```{r, warning=F}
# model comparison
accu_auto <- accuracy(dispute_ar_auto)
accu_auto2 <- accuracy(dispute_diff_xts.fit)
accu_ets <- accuracy(dispute_fitets)
accu_hw <- accuracy(dispute_forecasts2)
accu_ann <- accuracy(dispute_ann)
accu_stru <- accuracy(dispute_struct_forec)

model_compare <- data.frame(rbind(accu_auto, accu_auto2, accu_ets, accu_hw, accu_ann, accu_stru))
rownames(model_compare) <- c('Auto ARIMA(0 mean)', 'Auto ARIMA', 'ETS','HoltWinters', 'Neural Net', 'Structural')
kableExtra::kable(model_compare, format='markdown')
```

# MULTIVARIATE
```{r}
xy = data.frame(combined_diff[,1], combined_diff[,2], combined_diff[,3],combined_diff[,4],combined_diff[,5],combined_diff[,6])
# Create a daily Date object - helps my work on dates
inds <- seq(as.Date("2020-03-13"), as.Date("2020-05-19"), by = "day")
mymts = ts(xy, start = c(2020, as.numeric(format(inds[1], "%j"))),
           frequency = 365)
plot(mymts)
```
```{r}
apply(combined_diff, 2, adf.test)
autoplot(ts(na.omit(combined_diff))) +
  ggtitle('Time Series Plot of the stationary Time-Series') +
  theme(plot.title = element_text(hjust = 0.5)) #for centering the text
```

